{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nfrom torch import nn\nfrom torch import Tensor\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom PIL import Image\nfrom torchvision.transforms import Compose, Resize, ToTensor","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:48.58704Z","iopub.execute_input":"2022-03-02T05:02:48.587441Z","iopub.status.idle":"2022-03-02T05:02:48.596362Z","shell.execute_reply.started":"2022-03-02T05:02:48.587371Z","shell.execute_reply":"2022-03-02T05:02:48.594766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = '/kaggle/input/plant-seedlings-classification/train'\ntest_dir = '/kaggle/input/plant-seedlings-classification/test'","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:48.598464Z","iopub.execute_input":"2022-03-02T05:02:48.599721Z","iopub.status.idle":"2022-03-02T05:02:48.608802Z","shell.execute_reply.started":"2022-03-02T05:02:48.599619Z","shell.execute_reply":"2022-03-02T05:02:48.607258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_map = {\n 'Black-grass': 0,\n 'Charlock': 1,\n 'Cleavers': 2,\n 'Common Chickweed': 3,\n 'Common wheat': 4,\n 'Fat Hen': 5,\n 'Loose Silky-bent': 6,\n 'Maize': 7,\n 'Scentless Mayweed': 8,\n 'Shepherds Purse': 9,\n 'Small-flowered Cranesbill': 10,\n 'Sugar beet': 11\n}\n\nid_to_class = {\n 0: 'Black-grass',\n 1: 'Charlock',\n 2: 'Cleavers',\n 3: 'Common Chickweed',\n 4: 'Common wheat',\n 5: 'Fat Hen',\n 6: 'Loose Silky-bent',\n 7: 'Maize',\n 8: 'Scentless Mayweed',\n 9: 'Shepherds Purse',\n 10: 'Small-flowered Cranesbill',\n 11: 'Sugar beet'\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:48.610996Z","iopub.execute_input":"2022-03-02T05:02:48.611776Z","iopub.status.idle":"2022-03-02T05:02:48.620849Z","shell.execute_reply.started":"2022-03-02T05:02:48.611728Z","shell.execute_reply":"2022-03-02T05:02:48.619775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = 224\nbatch_size = 16\nepochs = 50\nCHECKPOINT_PATH = \"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:48.624074Z","iopub.execute_input":"2022-03-02T05:02:48.625021Z","iopub.status.idle":"2022-03-02T05:02:48.631272Z","shell.execute_reply.started":"2022-03-02T05:02:48.624893Z","shell.execute_reply":"2022-03-02T05:02:48.630169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomSizedCrop(image_size),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nvalid_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.CenterCrop(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.CenterCrop(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:48.633002Z","iopub.execute_input":"2022-03-02T05:02:48.633579Z","iopub.status.idle":"2022-03-02T05:02:48.650239Z","shell.execute_reply.started":"2022-03-02T05:02:48.633533Z","shell.execute_reply":"2022-03-02T05:02:48.649003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset\n\ntrain_dataset = ImageFolder(train_dir, transform = train_transforms)\nvalid_size = 0.10\n\n# Train-Valid split\nnum_train = len(train_dataset)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:48.652259Z","iopub.execute_input":"2022-03-02T05:02:48.653004Z","iopub.status.idle":"2022-03-02T05:02:49.599763Z","shell.execute_reply.started":"2022-03-02T05:02:48.652956Z","shell.execute_reply":"2022-03-02T05:02:49.598751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Loader\ntrain_loader = DataLoader(train_dataset,batch_size=16,sampler=train_sampler)\nvalid_loader = DataLoader(train_dataset, batch_size =16, sampler=valid_sampler)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:49.601608Z","iopub.execute_input":"2022-03-02T05:02:49.60221Z","iopub.status.idle":"2022-03-02T05:02:49.608475Z","shell.execute_reply.started":"2022-03-02T05:02:49.602163Z","shell.execute_reply":"2022-03-02T05:02:49.607518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device:\", device)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:49.610218Z","iopub.execute_input":"2022-03-02T05:02:49.610842Z","iopub.status.idle":"2022-03-02T05:02:49.669443Z","shell.execute_reply.started":"2022-03-02T05:02:49.610798Z","shell.execute_reply":"2022-03-02T05:02:49.668283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import pytorch_lightning as pl\nexcept ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n    !pip install --quiet pytorch-lightning>=1.4\n    import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:49.671307Z","iopub.execute_input":"2022-03-02T05:02:49.672136Z","iopub.status.idle":"2022-03-02T05:02:52.199362Z","shell.execute_reply.started":"2022-03-02T05:02:49.672088Z","shell.execute_reply":"2022-03-02T05:02:52.198115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_to_patch(x, patch_size, flatten_channels=True):\n    \"\"\"\n    Inputs:\n        x - torch.Tensor representing the image of shape [B, C, H, W]\n        patch_size - Number of pixels per dimension of the patches (integer)\n        flatten_channels - If True, the patches will be returned in a flattened format\n                           as a feature vector instead of a image grid.\n    \"\"\"\n    B, C, H, W = x.shape\n    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n    if flatten_channels:\n        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:52.205137Z","iopub.execute_input":"2022-03-02T05:02:52.205834Z","iopub.status.idle":"2022-03-02T05:02:52.219192Z","shell.execute_reply.started":"2022-03-02T05:02:52.205774Z","shell.execute_reply":"2022-03-02T05:02:52.217878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionBlock(nn.Module):\n\n    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n        \"\"\"\n        Inputs:\n            embed_dim - Dimensionality of input and attention feature vectors\n            hidden_dim - Dimensionality of hidden layer in feed-forward network\n                         (usually 2-4x larger than embed_dim)\n            num_heads - Number of heads to use in the Multi-Head Attention block\n            dropout - Amount of dropout to apply in the feed-forward network\n        \"\"\"\n        super().__init__()\n\n        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n        self.linear = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n\n\n    def forward(self, x):\n        inp_x = self.layer_norm_1(x)\n        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n        x = x + self.linear(self.layer_norm_2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:52.221131Z","iopub.execute_input":"2022-03-02T05:02:52.22252Z","iopub.status.idle":"2022-03-02T05:02:52.236647Z","shell.execute_reply.started":"2022-03-02T05:02:52.222465Z","shell.execute_reply":"2022-03-02T05:02:52.235542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n\n    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n        \"\"\"\n        Inputs:\n            embed_dim - Dimensionality of the input feature vectors to the Transformer\n            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n                         within the Transformer\n            num_channels - Number of channels of the input (3 for RGB)\n            num_heads - Number of heads to use in the Multi-Head Attention block\n            num_layers - Number of layers to use in the Transformer\n            num_classes - Number of classes to predict\n            patch_size - Number of pixels that the patches have per dimension\n            num_patches - Maximum number of patches an image can have\n            dropout - Amount of dropout to apply in the feed-forward network and\n                      on the input encoding\n        \"\"\"\n        super().__init__()\n\n        self.patch_size = patch_size\n\n        # Layers/Networks\n        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n        self.dropout = nn.Dropout(dropout)\n\n        # Parameters/Embeddings\n        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n\n\n    def forward(self, x):\n        # Preprocess input\n        x = img_to_patch(x, self.patch_size)\n        B, T, _ = x.shape\n        x = self.input_layer(x)\n\n        # Add CLS token and positional encoding\n        cls_token = self.cls_token.repeat(B, 1, 1)\n        x = torch.cat([cls_token, x], dim=1)\n        x = x + self.pos_embedding[:,:T+1]\n\n        # Apply Transforrmer\n        x = self.dropout(x)\n        x = x.transpose(0, 1)\n        x = self.transformer(x)\n\n        # Perform classification prediction\n        cls = x[0]\n        out = self.mlp_head(cls)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:52.238998Z","iopub.execute_input":"2022-03-02T05:02:52.239302Z","iopub.status.idle":"2022-03-02T05:02:52.260733Z","shell.execute_reply.started":"2022-03-02T05:02:52.239265Z","shell.execute_reply":"2022-03-02T05:02:52.259452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ViT(pl.LightningModule):\n\n    def __init__(self, model_kwargs, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = VisionTransformer(**model_kwargs)\n        self.example_input_array = next(iter(train_loader))[0]\n\n    def forward(self, x):\n        return self.model(x)\n\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n        return [optimizer], [lr_scheduler]\n\n    def _calculate_loss(self, batch, mode=\"train\"):\n        imgs, labels = batch\n        preds = self.model(imgs)\n        loss = F.cross_entropy(preds, labels)\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n\n        self.log(f'{mode}_loss', loss)\n        self.log(f'{mode}_acc', acc)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._calculate_loss(batch, mode=\"train\")\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        self._calculate_loss(batch, mode=\"val\")\n\n    def test_step(self, batch, batch_idx):\n        self._calculate_loss(batch, mode=\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:52.265384Z","iopub.execute_input":"2022-03-02T05:02:52.265805Z","iopub.status.idle":"2022-03-02T05:02:52.283489Z","shell.execute_reply.started":"2022-03-02T05:02:52.265678Z","shell.execute_reply":"2022-03-02T05:02:52.282346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(**kwargs):\n    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n                         gpus=1 if str(device)==\"cuda:0\" else 0,\n                         max_epochs=epochs,\n                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n                                    LearningRateMonitor(\"epoch\")],\n                         progress_bar_refresh_rate=1)\n    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n\n    # Check whether pretrained model exists. If yes, load it and skip training\n    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n    if os.path.isfile(pretrained_filename):\n        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n    else:\n        pl.seed_everything(42) # To be reproducable\n        model = ViT(**kwargs)\n        trainer.fit(model, train_loader, valid_loader)\n        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n\n    # Test best model on validation and test set\n    train_result = trainer.test(model, test_dataloaders=train_loader, verbose=False)\n    val_result = trainer.test(model, test_dataloaders=valid_loader, verbose=False)\n    #test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n    result = {\"train\": train_result, \"val\": val_result} #\"test\": test_result}\n\n    return model, result","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:52.286202Z","iopub.execute_input":"2022-03-02T05:02:52.286762Z","iopub.status.idle":"2022-03-02T05:02:52.300842Z","shell.execute_reply.started":"2022-03-02T05:02:52.286679Z","shell.execute_reply":"2022-03-02T05:02:52.299504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, results = train_model(model_kwargs={\n                                'embed_dim': 256,\n                                'hidden_dim': 512,\n                                'num_heads': 8,\n                                'num_layers': 6,\n                                'patch_size': 16,\n                                'num_channels': 3,\n                                'num_patches': 196,\n                                'num_classes': 12,\n                                'dropout': 0.25\n                            },\n                            lr=3e-4)\nprint(\"ViT results\", results)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T05:02:52.302423Z","iopub.execute_input":"2022-03-02T05:02:52.303059Z","iopub.status.idle":"2022-03-02T06:20:50.027423Z","shell.execute_reply.started":"2022-03-02T05:02:52.30301Z","shell.execute_reply":"2022-03-02T06:20:50.026044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Create_Directory_DataFrame(basedir):\n    df = pd.DataFrame(columns=['Location'])\n    # basedir\n    for location in os.listdir(basedir+'/'):\n        df = df.append({'Location':basedir+'/'+location},ignore_index=True)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:20:50.029531Z","iopub.execute_input":"2022-03-02T06:20:50.029891Z","iopub.status.idle":"2022-03-02T06:20:50.036657Z","shell.execute_reply.started":"2022-03-02T06:20:50.029844Z","shell.execute_reply":"2022-03-02T06:20:50.035391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred_class(img):\n    # transform images\n    img_tens = test_transforms(img)\n    img_im = img_tens.unsqueeze(0).cuda() \n    uinput = Variable(img_im)\n    uinput = uinput.to(device)\n    out = model(uinput)\n    # convert image to numpy format in cpu and snatching max prediction score class index\n    index = out.data.cpu().numpy().argmax()    \n    return index","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:23:03.307282Z","iopub.execute_input":"2022-03-02T06:23:03.307572Z","iopub.status.idle":"2022-03-02T06:23:03.313869Z","shell.execute_reply.started":"2022-03-02T06:23:03.307539Z","shell.execute_reply":"2022-03-02T06:23:03.312606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nmodel.cuda()\n\ntest_df = create_directory_dataFrame(test_dir)\n\nsubmission = pd.DataFrame(columns=['file', 'species'])\n\nfor i, image in enumerate(test_df['Location']):\n    img = Image.open(image)\n    image = image.split('/')[-1]\n    index = pred_class(img)\n    pred = id_to_class[index]\n    \n    submission = submission.append({'file': image, 'species': pred}, ignore_index=True)\n    \nsubmission.to_csv('ViT_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:27:01.153477Z","iopub.execute_input":"2022-03-02T06:27:01.153844Z","iopub.status.idle":"2022-03-02T06:27:16.529838Z","shell.execute_reply.started":"2022-03-02T06:27:01.153811Z","shell.execute_reply":"2022-03-02T06:27:16.528861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save({\n            'epoch': epochs,\n            'model_state_dict': model.state_dict(),\n            }, 'vision_transformer.pth')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:24:00.245418Z","iopub.execute_input":"2022-03-02T06:24:00.245762Z","iopub.status.idle":"2022-03-02T06:24:00.287427Z","shell.execute_reply.started":"2022-03-02T06:24:00.245674Z","shell.execute_reply":"2022-03-02T06:24:00.286476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}