{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Install Required Packages","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:51.263406Z","iopub.execute_input":"2022-03-04T13:39:51.264481Z","iopub.status.idle":"2022-03-04T13:40:08.644975Z","shell.execute_reply.started":"2022-03-04T13:39:51.264358Z","shell.execute_reply":"2022-03-04T13:40:08.644123Z"}}},{"cell_type":"code","source":"!pip install -q transformers pytorch-lightning","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torchvision\nimport numpy as np\nimport timm\nimport os\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport pandas as pd\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.autograd import Variable\nimport operator\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom PIL import Image\nimport torchvision.transforms as transforms \nimport seaborn as sns\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.metrics import classification_report\nimport matplotlib.ticker as ticker\nimport itertools","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:08.648315Z","iopub.execute_input":"2022-03-04T13:40:08.648573Z","iopub.status.idle":"2022-03-04T13:40:11.901023Z","shell.execute_reply.started":"2022-03-04T13:40:08.64854Z","shell.execute_reply":"2022-03-04T13:40:11.900229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:11.902671Z","iopub.execute_input":"2022-03-04T13:40:11.902931Z","iopub.status.idle":"2022-03-04T13:40:12.176079Z","shell.execute_reply.started":"2022-03-04T13:40:11.902886Z","shell.execute_reply":"2022-03-04T13:40:12.175409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = '/kaggle/input/plant-seedlings-classification/train'\ntest_dir = '/kaggle/input/plant-seedlings-classification/test'","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:12.177388Z","iopub.execute_input":"2022-03-04T13:40:12.177644Z","iopub.status.idle":"2022-03-04T13:40:12.183546Z","shell.execute_reply.started":"2022-03-04T13:40:12.177608Z","shell.execute_reply":"2022-03-04T13:40:12.182818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_map = {\n 'Black-grass': 0,\n 'Charlock': 1,\n 'Cleavers': 2,\n 'Common Chickweed': 3,\n 'Common wheat': 4,\n 'Fat Hen': 5,\n 'Loose Silky-bent': 6,\n 'Maize': 7,\n 'Scentless Mayweed': 8,\n 'Shepherds Purse': 9,\n 'Small-flowered Cranesbill': 10,\n 'Sugar beet': 11\n}\n\nid_to_class = {\n 0: 'Black-grass',\n 1: 'Charlock',\n 2: 'Cleavers',\n 3: 'Common Chickweed',\n 4: 'Common wheat',\n 5: 'Fat Hen',\n 6: 'Loose Silky-bent',\n 7: 'Maize',\n 8: 'Scentless Mayweed',\n 9: 'Shepherds Purse',\n 10: 'Small-flowered Cranesbill',\n 11: 'Sugar beet'\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:12.187357Z","iopub.execute_input":"2022-03-04T13:40:12.187786Z","iopub.status.idle":"2022-03-04T13:40:12.195282Z","shell.execute_reply.started":"2022-03-04T13:40:12.187757Z","shell.execute_reply":"2022-03-04T13:40:12.194541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\nepochs = 50\nCHECKPOINT_PATH = \"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:12.197407Z","iopub.execute_input":"2022-03-04T13:40:12.197822Z","iopub.status.idle":"2022-03-04T13:40:12.20358Z","shell.execute_reply.started":"2022-03-04T13:40:12.197654Z","shell.execute_reply":"2022-03-04T13:40:12.202832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:12.205246Z","iopub.execute_input":"2022-03-04T13:40:12.205576Z","iopub.status.idle":"2022-03-04T13:40:12.922512Z","shell.execute_reply.started":"2022-03-04T13:40:12.205541Z","shell.execute_reply":"2022-03-04T13:40:12.921844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\ntrain_transforms = transforms.Compose(\n        [\n            transforms.RandomResizedCrop(feature_extractor.size),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]\n    )\n\nval_transforms = transforms.Compose(\n        [\n            transforms.Resize(feature_extractor.size),\n            transforms.CenterCrop(feature_extractor.size),\n            transforms.ToTensor(),\n            normalize,\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:12.923903Z","iopub.execute_input":"2022-03-04T13:40:12.924391Z","iopub.status.idle":"2022-03-04T13:40:12.931499Z","shell.execute_reply.started":"2022-03-04T13:40:12.924349Z","shell.execute_reply":"2022-03-04T13:40:12.930722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset\n\ntrain_dataset = ImageFolder(train_dir, transform = train_transforms)\nvalid_size = 0.10\n\n# Train-Valid split\nnum_train = len(train_dataset)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:12.932816Z","iopub.execute_input":"2022-03-04T13:40:12.933246Z","iopub.status.idle":"2022-03-04T13:40:14.015255Z","shell.execute_reply.started":"2022-03-04T13:40:12.933211Z","shell.execute_reply":"2022-03-04T13:40:14.014337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\nval_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\nval_dataloader.dataset.transforms = val_transforms\ntest_dataloader = val_dataloader","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:14.016711Z","iopub.execute_input":"2022-03-04T13:40:14.016979Z","iopub.status.idle":"2022-03-04T13:40:14.022189Z","shell.execute_reply.started":"2022-03-04T13:40:14.016932Z","shell.execute_reply":"2022-03-04T13:40:14.02125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device:\", device)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:14.023662Z","iopub.execute_input":"2022-03-04T13:40:14.024137Z","iopub.status.idle":"2022-03-04T13:40:14.076896Z","shell.execute_reply.started":"2022-03-04T13:40:14.0241Z","shell.execute_reply":"2022-03-04T13:40:14.076146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nprint(batch[0].shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:14.078339Z","iopub.execute_input":"2022-03-04T13:40:14.078612Z","iopub.status.idle":"2022-03-04T13:40:14.771166Z","shell.execute_reply.started":"2022-03-04T13:40:14.078579Z","shell.execute_reply":"2022-03-04T13:40:14.770479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom transformers import ViTForImageClassification, AdamW\nimport torch.nn as nn\n\nclass ViTLightningModule(pl.LightningModule):\n    def __init__(self, num_labels=10):\n        super(ViTLightningModule, self).__init__()\n        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n                                                             ignore_mismatched_sizes=True,\n                                                              num_labels=12,\n                                                              id2label=id_to_class,\n                                                              label2id=class_map)\n\n    def forward(self, pixel_values):\n        outputs = self.vit(pixel_values=pixel_values)\n        return outputs.logits\n        \n    def common_step(self, batch, batch_idx):\n        pixel_values, labels = batch\n        logits = self(pixel_values)\n\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(logits, labels)\n        predictions = logits.argmax(-1)\n        correct = (predictions == labels).sum().item()\n        accuracy = correct/pixel_values.shape[0]\n\n        return loss, accuracy\n      \n    def training_step(self, batch, batch_idx):\n        loss, accuracy = self.common_step(batch, batch_idx)     \n        # logs metrics for each training_step,\n        # and the average across the epoch\n        self.log(\"training_loss\", loss)\n        self.log(\"training_accuracy\", accuracy)\n\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        loss, accuracy = self.common_step(batch, batch_idx)     \n        self.log(\"validation_loss\", loss, on_epoch=True)\n        self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        loss, accuracy = self.common_step(batch, batch_idx)     \n\n        return loss\n\n    def configure_optimizers(self):\n        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n        # not require weight_decay but just using AdamW out-of-the-box works fine\n        return AdamW(self.parameters(), lr=5e-5)\n\n    def train_dataloader(self):\n        return train_dataloader\n\n    def val_dataloader(self):\n        return val_dataloader\n\n    def test_dataloader(self):\n        return test_dataloader","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:14.77569Z","iopub.execute_input":"2022-03-04T13:40:14.777811Z","iopub.status.idle":"2022-03-04T13:40:19.387653Z","shell.execute_reply.started":"2022-03-04T13:40:14.777771Z","shell.execute_reply":"2022-03-04T13:40:19.386907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import EarlyStopping\n\n# for early stopping, see https://pytorch-lightning.readthedocs.io/en/1.0.0/early_stopping.html?highlight=early%20stopping\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=20,\n    strict=False,\n    verbose=True,\n    mode='min'\n)\n\nmodel = ViTLightningModule()\ntrainer = Trainer(gpus=1, callbacks=[EarlyStopping(monitor='validation_loss')])\ntrainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:19.390188Z","iopub.execute_input":"2022-03-04T13:40:19.390379Z","iopub.status.idle":"2022-03-04T14:22:14.592122Z","shell.execute_reply.started":"2022-03-04T13:40:19.390355Z","shell.execute_reply":"2022-03-04T14:22:14.591409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_directory_dataFrame(basedir):\n    df = pd.DataFrame(columns=['Location'])\n    # basedir\n    for location in os.listdir(basedir+'/'):\n        df = df.append({'Location':basedir+'/'+location},ignore_index=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-04T14:22:14.593371Z","iopub.execute_input":"2022-03-04T14:22:14.59364Z","iopub.status.idle":"2022-03-04T14:22:14.599262Z","shell.execute_reply.started":"2022-03-04T14:22:14.593607Z","shell.execute_reply":"2022-03-04T14:22:14.598609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred_class(img):\n    # transform images\n    img_tens = val_transforms(img)\n    img_im = img_tens.unsqueeze(0).cuda() \n    uinput = Variable(img_im)\n    uinput = uinput.to(device)\n    out = model(uinput)\n    # convert image to numpy format in cpu and snatching max prediction score class index\n    index = out.data.cpu().numpy().argmax()    \n    return index","metadata":{"execution":{"iopub.status.busy":"2022-03-04T14:22:14.600366Z","iopub.execute_input":"2022-03-04T14:22:14.60084Z","iopub.status.idle":"2022-03-04T14:22:14.623112Z","shell.execute_reply.started":"2022-03-04T14:22:14.600803Z","shell.execute_reply":"2022-03-04T14:22:14.622258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nmodel.cuda()\n\ntest_df = create_directory_dataFrame(test_dir)\n\nsubmission = pd.DataFrame(columns=['file', 'species'])\n\nfor i, image in enumerate(test_df['Location']):\n    img = Image.open(image)\n    image = image.split('/')[-1]\n    index = pred_class(img)\n    pred = id_to_class[index]\n    \n    submission = submission.append({'file': image, 'species': pred}, ignore_index=True)\n    \nsubmission.to_csv('vit-base-patch16-384_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T14:22:14.624586Z","iopub.execute_input":"2022-03-04T14:22:14.625123Z","iopub.status.idle":"2022-03-04T14:22:52.716528Z","shell.execute_reply.started":"2022-03-04T14:22:14.62508Z","shell.execute_reply":"2022-03-04T14:22:52.715795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save({\n            'epoch': epochs,\n            'model_state_dict': model.state_dict(),\n            }, 'vit-base-patch16-224-in21k.pth')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-04T14:22:52.717613Z","iopub.execute_input":"2022-03-04T14:22:52.71784Z","iopub.status.idle":"2022-03-04T14:22:53.221078Z","shell.execute_reply.started":"2022-03-04T14:22:52.717807Z","shell.execute_reply":"2022-03-04T14:22:53.220314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}