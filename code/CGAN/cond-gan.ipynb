{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-03T12:45:08.769363Z","iopub.execute_input":"2022-03-03T12:45:08.769961Z","iopub.status.idle":"2022-03-03T12:45:08.792669Z","shell.execute_reply.started":"2022-03-03T12:45:08.769858Z","shell.execute_reply":"2022-03-03T12:45:08.791961Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nimport torch.nn as nn\nfrom torchvision.utils import save_image\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dset\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport torchvision.utils as vutils\nimport numpy as np\nimport pandas as pd\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:08.795651Z","iopub.execute_input":"2022-03-03T12:45:08.796151Z","iopub.status.idle":"2022-03-03T12:45:10.218060Z","shell.execute_reply.started":"2022-03-03T12:45:08.796115Z","shell.execute_reply":"2022-03-03T12:45:10.217063Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataroot = '../input/plant-seedlings-classification/train'\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:10.219739Z","iopub.execute_input":"2022-03-03T12:45:10.219965Z","iopub.status.idle":"2022-03-03T12:45:10.226330Z","shell.execute_reply.started":"2022-03-03T12:45:10.219936Z","shell.execute_reply":"2022-03-03T12:45:10.225397Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"img_shape = 128\nlatent_dim = 100\nbatch_size = 16\nsample_interval = 400","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:10.227619Z","iopub.execute_input":"2022-03-03T12:45:10.228196Z","iopub.status.idle":"2022-03-03T12:45:10.235938Z","shell.execute_reply.started":"2022-03-03T12:45:10.228151Z","shell.execute_reply":"2022-03-03T12:45:10.235155Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset = dset.ImageFolder(root=dataroot,\n                           transform=transforms.Compose([\n                               transforms.Resize(img_shape),\n                               transforms.CenterCrop(img_shape),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:10.240233Z","iopub.execute_input":"2022-03-03T12:45:10.240487Z","iopub.status.idle":"2022-03-03T12:45:11.780024Z","shell.execute_reply.started":"2022-03-03T12:45:10.240458Z","shell.execute_reply":"2022-03-03T12:45:11.779344Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset.class_to_idx","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:11.783733Z","iopub.execute_input":"2022-03-03T12:45:11.784390Z","iopub.status.idle":"2022-03-03T12:45:11.795965Z","shell.execute_reply.started":"2022-03-03T12:45:11.784345Z","shell.execute_reply":"2022-03-03T12:45:11.795049Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Plot some training images\nreal = next(iter(dataloader))\nplt.figure(figsize=(10,10))\nplt.title(\"Example Images\")\nplt.imshow(np.transpose(vutils.make_grid(real[0].to(device), padding=2, normalize=True).cpu(),(1,2,0)))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:11.797338Z","iopub.execute_input":"2022-03-03T12:45:11.797573Z","iopub.status.idle":"2022-03-03T12:45:12.957830Z","shell.execute_reply.started":"2022-03-03T12:45:11.797545Z","shell.execute_reply":"2022-03-03T12:45:12.956818Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(real[0].shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:12.959339Z","iopub.execute_input":"2022-03-03T12:45:12.959648Z","iopub.status.idle":"2022-03-03T12:45:12.965591Z","shell.execute_reply.started":"2022-03-03T12:45:12.959610Z","shell.execute_reply":"2022-03-03T12:45:12.964726Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"n_classes = 12\nembedding_dim = 100","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:12.967290Z","iopub.execute_input":"2022-03-03T12:45:12.967605Z","iopub.status.idle":"2022-03-03T12:45:12.976108Z","shell.execute_reply.started":"2022-03-03T12:45:12.967552Z","shell.execute_reply":"2022-03-03T12:45:12.975497Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:12.977155Z","iopub.execute_input":"2022-03-03T12:45:12.977538Z","iopub.status.idle":"2022-03-03T12:45:12.986966Z","shell.execute_reply.started":"2022-03-03T12:45:12.977506Z","shell.execute_reply":"2022-03-03T12:45:12.986229Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def create_noise(samples):\n    return torch.randn(samples, latent_dim+10, device=device)\nfinal_noise = create_noise(64).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:12.988721Z","iopub.execute_input":"2022-03-03T12:45:12.989244Z","iopub.status.idle":"2022-03-03T12:45:13.000398Z","shell.execute_reply.started":"2022-03-03T12:45:12.989205Z","shell.execute_reply":"2022-03-03T12:45:12.999738Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n     \n        self.label_conditioned_generator = nn.Sequential(nn.Embedding(n_classes, embedding_dim),\n                      nn.Linear(embedding_dim, 16))\n        \n    \n        self.latent = nn.Sequential(nn.Linear(latent_dim, 4*4*512),\n                                   nn.LeakyReLU(0.2, inplace=True))\n           \n\n        self.model = nn.Sequential(nn.ConvTranspose2d(513, 64*8, 4, 2, 1, bias=False),\n                      nn.BatchNorm2d(64*8, momentum=0.1,  eps=0.8),\n                      nn.ReLU(True),\n\n                      nn.ConvTranspose2d(64*8, 64*4, 4, 2, 1,bias=False),\n                      nn.BatchNorm2d(64*4, momentum=0.1,  eps=0.8),\n                      nn.ReLU(True), \n                                   \n                      nn.ConvTranspose2d(64*4, 64*2, 4, 2, 1,bias=False),\n                      nn.BatchNorm2d(64*2, momentum=0.1,  eps=0.8),\n                      nn.ReLU(True),\n                                   \n                      nn.ConvTranspose2d(64*2, 64*1, 4, 2, 1,bias=False),\n                      nn.BatchNorm2d(64*1, momentum=0.1,  eps=0.8),\n                      nn.ReLU(True),\n                                   \n                      nn.ConvTranspose2d(64*1, 3, 4, 2, 1, bias=False),\n                      nn.Tanh())\n\n    def forward(self, inputs):\n        noise_vector, label = inputs\n        label_output = self.label_conditioned_generator(label)\n        label_output = label_output.view(-1, 1, 4, 4)\n        latent_output = self.latent(noise_vector)\n        latent_output = latent_output.view(-1, 512,4,4)\n        concat = torch.cat((latent_output, label_output), dim=1)\n#         print(concat.shape)\n        image = self.model(concat)\n        #print(image.size())\n        return image","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:13.534618Z","iopub.execute_input":"2022-03-03T12:45:13.535049Z","iopub.status.idle":"2022-03-03T12:45:13.549187Z","shell.execute_reply.started":"2022-03-03T12:45:13.535018Z","shell.execute_reply":"2022-03-03T12:45:13.548388Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n    \n        self.label_condition_disc = nn.Sequential(nn.Embedding(n_classes, embedding_dim),\n                      nn.Linear(embedding_dim, 3*img_shape*img_shape))\n             \n        self.model = nn.Sequential(nn.Conv2d(6, 64, 4, 2, 1, bias=False),\n                      nn.LeakyReLU(0.2, inplace=True),\n                      nn.Conv2d(64, 64*2, 4, 3, 2, bias=False),\n                      nn.BatchNorm2d(64*2, momentum=0.1,  eps=0.8),\n                      nn.LeakyReLU(0.2, inplace=True),\n                      nn.Conv2d(64*2, 64*4, 4, 3,2, bias=False),\n                      nn.BatchNorm2d(64*4, momentum=0.1,  eps=0.8),\n                      nn.LeakyReLU(0.2, inplace=True),\n                      nn.Conv2d(64*4, 64*8, 4, 3, 2, bias=False),\n                      nn.BatchNorm2d(64*8, momentum=0.1,  eps=0.8),\n                      nn.LeakyReLU(0.2, inplace=True), \n                      nn.Flatten(),\n                      nn.Dropout(0.4),\n                      nn.Linear(4608, 1),\n                      nn.Sigmoid()\n                     )\n\n    def forward(self, inputs):\n        img, label = inputs\n        label_output = self.label_condition_disc(label)\n        label_output = label_output.view(-1, 3, img_shape, img_shape)\n#         print(img.shape)\n#         print(label_output.shape)\n#         print(\"done\")\n        concat = torch.cat((img, label_output), dim=1)\n        #print(concat.size())\n        output = self.model(concat)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:45:15.207626Z","iopub.execute_input":"2022-03-03T12:45:15.208093Z","iopub.status.idle":"2022-03-03T12:45:15.219379Z","shell.execute_reply.started":"2022-03-03T12:45:15.208032Z","shell.execute_reply":"2022-03-03T12:45:15.218741Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"generator = Generator().to(device)\ngenerator.apply(weights_init)\n\ndiscriminator = Discriminator().to(device)\ndiscriminator.apply(weights_init)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:07:46.86918Z","iopub.execute_input":"2022-03-03T04:07:46.869624Z","iopub.status.idle":"2022-03-03T04:07:47.027746Z","shell.execute_reply.started":"2022-03-03T04:07:46.869588Z","shell.execute_reply":"2022-03-03T04:07:47.027079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.BCELoss(reduction='mean')\nD_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\nG_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n\ndef generator_loss(label, fake_output):\n    gen_loss = binary_cross_entropy(label, fake_output)\n    #print(gen_loss)\n    return gen_loss\n\ndef discriminator_loss(label, output):\n    disc_loss = binary_cross_entropy(label, output)\n    #print(total_loss)\n    return disc_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:07:47.029644Z","iopub.execute_input":"2022-03-03T04:07:47.030085Z","iopub.status.idle":"2022-03-03T04:07:47.036713Z","shell.execute_reply.started":"2022-03-03T04:07:47.030044Z","shell.execute_reply":"2022-03-03T04:07:47.035972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_noise = torch.randn(batch_size, latent_dim, device=device)  \nfinal_noise = final_noise.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:07:47.03789Z","iopub.execute_input":"2022-03-03T04:07:47.038456Z","iopub.status.idle":"2022-03-03T04:07:47.048836Z","shell.execute_reply.started":"2022-03-03T04:07:47.038419Z","shell.execute_reply":"2022-03-03T04:07:47.048161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# anim = []\n# num_epochs = 100\n# for epoch in range(num_epochs): \n\n#     D_loss_list, G_loss_list = [], []\n\n    \n#     for index, (real_images, labels) in enumerate(dataloader):\n#         D_optimizer.zero_grad()\n#         real_images = real_images.to(device)\n#         labels = labels.to(device)\n#         labels = labels.unsqueeze(1).long()\n\n      \n#         real_target = Variable(torch.ones(real_images.size(0), 1).to(device))\n#         fake_target = Variable(torch.zeros(real_images.size(0), 1).to(device))\n      \n#         D_real_loss = criterion(discriminator((real_images, labels)), real_target)\n#         # print(discriminator(real_images))\n#         #D_real_loss.backward()\n    \n#         noise_vector = torch.randn(real_images.size(0), latent_dim, device=device)  \n#         noise_vector = noise_vector.to(device)\n        \n       \n#         generated_image = generator((noise_vector, labels))\n#         output = discriminator((generated_image.detach(), labels))\n#         D_fake_loss = criterion(output,  fake_target)\n\n    \n#         # train with fake\n#         #D_fake_loss.backward()\n      \n#         D_total_loss = (D_real_loss + D_fake_loss) / 2\n#         D_loss_list.append(D_total_loss)\n      \n#         D_total_loss.backward()\n#         D_optimizer.step()\n\n#         # Train generator with real labels\n#         G_optimizer.zero_grad()\n#         G_loss = criterion(discriminator((generated_image, labels)), real_target)\n#         G_loss_list.append(G_loss)\n\n#         G_loss.backward()\n#         G_optimizer.step()\n        \n#         if index%100 == 0: \n#             final_images = generator((final_noise, labels)).detach()\n#             final_images = vutils.make_grid(final_images.to(device), padding=2, normalize=True)\n#             anim.append(final_images)\n        \n#     print(f\"Epoch: {epoch+1} , Gen Loss: {G_loss} , Disc Loss: { D_total_loss}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-03T04:07:47.050193Z","iopub.execute_input":"2022-03-03T04:07:47.050683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # save the generator model\n# from datetime import datetime\n# gen_filename = datetime.now().strftime('/kaggle/working/%d-%m-%y-%H_%M_cgan_genweights.h5')\n# disc_filename = datetime.now().strftime('/kaggle/working/%d-%m-%y-%H_%M_cgan_discweights.h5')\n# torch.save(generator, gen_filename)\n# torch.save(discriminator, disc_filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen256_filename = '../input/outputfile/02-03-22-18_34_cgan_genweights.h5'\n\ngen256 = torch.load(gen_256_filename, map_location=torch.device('cpu'))\ngen256.eval()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:59:46.841103Z","iopub.execute_input":"2022-03-03T12:59:46.841858Z","iopub.status.idle":"2022-03-03T12:59:46.902017Z","shell.execute_reply.started":"2022-03-03T12:59:46.841821Z","shell.execute_reply":"2022-03-03T12:59:46.900973Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"gen128_filename = '../input/outputfile/03-03-22-06_12_cgan_genweights.h5'\n\ngen128 = torch.load(gen128_filename, map_location=torch.device('cpu'))\ngen128.eval()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:46:47.635348Z","iopub.execute_input":"2022-03-03T12:46:47.635672Z","iopub.status.idle":"2022-03-03T12:46:48.347887Z","shell.execute_reply.started":"2022-03-03T12:46:47.635636Z","shell.execute_reply":"2022-03-03T12:46:48.346991Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"seed_type = os.listdir(\"/kaggle/input/plant-seedlings-classification/train\")","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:52:00.122018Z","iopub.execute_input":"2022-03-03T12:52:00.122721Z","iopub.status.idle":"2022-03-03T12:52:00.131168Z","shell.execute_reply.started":"2022-03-03T12:52:00.122671Z","shell.execute_reply":"2022-03-03T12:52:00.130127Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"label = 5\nbatch_size = 5\nlabels = torch.ones(batch_size,device=device) * label\nprint(labels)\nlabels = labels.to(device)\nlabels = labels.unsqueeze(1).long()\nnoise = torch.randn(batch_size, latent_dim, device=device).to(device)\nprint(noise.shape)\npredictions = gen256((noise, labels))\nimages = predictions.detach()\nimages = images.to(device)\nimages = vutils.make_grid(images, padding=2, normalize=True)\nplt.imshow(np.transpose(images.cpu(),(1,2,0)))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:02:33.326283Z","iopub.execute_input":"2022-03-03T13:02:33.326622Z","iopub.status.idle":"2022-03-03T13:02:34.039857Z","shell.execute_reply.started":"2022-03-03T13:02:33.326579Z","shell.execute_reply":"2022-03-03T13:02:34.038949Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"batch_size = 5\nlabel = 5\nlabels = torch.ones(batch_size,device=device) * label\nlabels = labels.to(device)\nlabels = labels.unsqueeze(1).long()\nnoise = torch.randn(batch_size, latent_dim, device=device).to(device)\nprint(labels,seed_type[label])\npredictions = gen128((noise, labels))\nimages = predictions.detach()\nimages = images.to(device)\nimages = vutils.make_grid(images, padding=2, normalize=True)\nplt.imshow(np.transpose(images.cpu(),(1,2,0)))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:03:20.740439Z","iopub.execute_input":"2022-03-03T13:03:20.740761Z","iopub.status.idle":"2022-03-03T13:03:21.083703Z","shell.execute_reply.started":"2022-03-03T13:03:20.740724Z","shell.execute_reply":"2022-03-03T13:03:21.082731Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def generate_images(model, num_img, n_classes=10):\n \n  # This is so all layers run in inference mode (batchnorm).\n    for label in range(n_classes):\n        root = '/kaggle/working/output_images/'+ str(label)\n        os.mkdir(root) if not os.path.exists(root) else None\n        \n        labels = torch.ones(1,device=device)* label\n        labels = labels.to(device)\n        labels = labels.unsqueeze(1).long()\n        for i in range(num_img):\n            noise = torch.randn(1, latent_dim, device=device).to(device)\n            predictions = gen((noise, labels))\n            images = predictions.detach()\n            images = vutils.make_grid(images.to(device), padding=2, normalize=True)\n            \n            save_image(images, os.path.join(root, f'image_{i}.png'))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:01:11.848202Z","iopub.execute_input":"2022-03-03T13:01:11.848516Z","iopub.status.idle":"2022-03-03T13:01:11.860168Z","shell.execute_reply.started":"2022-03-03T13:01:11.848483Z","shell.execute_reply":"2022-03-03T13:01:11.859088Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import save_image\nnum_examples_to_generate = 10\nlatent_dim = 100\nn_classes = 12\ngenerate_images(gen, num_examples_to_generate,  n_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root = '/kaggle/working/output_images/0/'\nimages = os.listdir(root)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = [os.path.join(root, img) for img in images]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}