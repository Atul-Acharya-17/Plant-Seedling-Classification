{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport itertools\nimport os\nfrom glob import glob\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import Input, Dense, Flatten, Activation, Dropout, BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Model, load_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.vis_utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2022-04-17T03:50:04.211628Z","iopub.execute_input":"2022-04-17T03:50:04.212056Z","iopub.status.idle":"2022-04-17T03:50:04.223979Z","shell.execute_reply.started":"2022-04-17T03:50:04.212025Z","shell.execute_reply":"2022-04-17T03:50:04.223316Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Load the data dir","metadata":{}},{"cell_type":"code","source":"# non segmented data\ndata_dir = \"../input/plant-seedlings-classification/\"\ntrain_dir = os.path.join(data_dir, \"train\")\ndata_dir = \"../input/plant-seedlings-classification/\"\ntest_dir = os.path.join(data_dir, \"test\")","metadata":{"execution":{"iopub.status.busy":"2022-04-17T03:32:09.483728Z","iopub.execute_input":"2022-04-17T03:32:09.484398Z","iopub.status.idle":"2022-04-17T03:32:09.489342Z","shell.execute_reply.started":"2022-04-17T03:32:09.484361Z","shell.execute_reply":"2022-04-17T03:32:09.488558Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# # segmented data\n# data_dir = \"../input/plant-seedling-segmented/plant-seedling-segmented/\"\n# train_dir = os.path.join(data_dir, \"seg_train\")\n# data_dir = \"../input/plant-seedling-segmented/plant-seedling-segmented/\"\n# test_dir = os.path.join(data_dir, \"seg_test\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image pre-processing: Segmentation","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/gaborvecsei/plant-seedlings-fun-with-computer-vision/notebook\n\nimport cv2\nimport numpy as np\n\ndef create_mask_for_plant(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n\n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return mask\n\ndef segment_plant(image):\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\n\ndef sharpen_image(image):\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n    return image_sharp","metadata":{"execution":{"iopub.status.busy":"2022-04-17T03:31:58.711832Z","iopub.execute_input":"2022-04-17T03:31:58.712415Z","iopub.status.idle":"2022-04-17T03:31:58.723343Z","shell.execute_reply.started":"2022-04-17T03:31:58.712366Z","shell.execute_reply":"2022-04-17T03:31:58.722646Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Load images and labels","metadata":{}},{"cell_type":"code","source":"# load train images\nimages, labels = [], []\n\nfor class_folder_name in os.listdir(train_dir):\n    class_folder_path = os.path.join(train_dir, class_folder_name)\n    \n    for image_path in glob(os.path.join(class_folder_path, \"*.png\")):\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        image_128 = cv2.resize(image, (128,128))\n        images.append(image_128)\n        labels.append(class_folder_name)\n\nimages = np.array(images)\nlabels = np.array(labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T03:32:11.576611Z","iopub.execute_input":"2022-04-17T03:32:11.576884Z","iopub.status.idle":"2022-04-17T03:32:13.850676Z","shell.execute_reply.started":"2022-04-17T03:32:11.576854Z","shell.execute_reply":"2022-04-17T03:32:13.849729Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# load non segmented\nimages_test = []\n\nfor image_path in glob(os.path.join(test_dir, \"*.png\")):\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    image_128 = cv2.resize(image, (128,128))\n    images_test.append(image_128)  \n\nimages_test = np.array(images_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # load segmented\n# images_test = []\n\n# for image_path in glob(os.path.join(test_dir, \"*.png\")):\n#     image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n#     image_299 = cv2.resize(image, (299, 299))\n#     image_segmented = segment_plant(image_299)\n#     image_sharpened = sharpen_image(image_segmented)\n#     images_test.append(image_sharpened)  \n\n# images_test = np.array(images_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define runtime params","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 175\nRANDOM_STATE = 11","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label pre-processing","metadata":{}},{"cell_type":"code","source":"# convert the text labels to numerical ones (dict)\ntext_label_to_num_label_dict = {v:i for i,v in enumerate(np.unique(labels))}\ntext_label_to_num_label_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert numerical ids to text labels (dict)\nnum_label_to_text_label_dict = {v: k for k, v in text_label_to_num_label_dict.items()}\nnum_label_to_text_label_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the text labels to numerical ones (list)\ntruth_labels = np.array([text_label_to_num_label_dict[x] for x in labels])\ntruth_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN model","metadata":{}},{"cell_type":"markdown","source":"### 1. Define model architecture","metadata":{}},{"cell_type":"code","source":"def create_model():\n    image = Input(shape=(128, 128, 3))\n\n    conv_1 = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu')(image)\n    bn_1 = BatchNormalization(axis=3)(conv_1)\n    act_1 = LeakyReLU(1/10)(bn_1)\n    \n    conv_2 = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu')(act_1)\n    bn_2 = BatchNormalization(axis=3)(conv_2)\n    act_2 = LeakyReLU(1/10)(bn_2)\n    \n    pool_1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(act_2)\n    \n    \n    conv_4 = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu')(pool_1)\n    bn_4 = BatchNormalization(axis=3)(conv_4)\n    act_4 = LeakyReLU(1/10)(bn_4)\n    \n    conv_5 = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu')(act_4)\n    bn_5 = BatchNormalization(axis=3)(conv_5)\n    act_5 = LeakyReLU(1/10)(bn_5)\n    \n    pool_2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(act_5)\n    \n\n    conv_7 = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu')(pool_2)\n    bn_7 = BatchNormalization(axis=3)(conv_7)\n    act_7 = LeakyReLU(1/10)(bn_7)\n    \n    conv_8 = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu')(act_7)\n    bn_8 = BatchNormalization(axis=3)(conv_8)\n    act_8 = LeakyReLU(1/10)(bn_8)\n    \n    pool_3 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(act_8)\n    \n\n    flat_1 = Flatten()(pool_3)\n    \n    drop_1 = Dropout(0.1)(flat_1)\n    dense_1 = Dense(128)(drop_1)\n    batch_n_1 = BatchNormalization(axis=-1)(dense_1)\n    a_1 = Activation(activation='tanh')(batch_n_1)\n    \n    drop_3 = Dropout(0.1)(a_1)\n    dense_3 = Dense(12)(drop_3)\n    batch_n_3 = BatchNormalization(axis=-1)(dense_3)\n    a_3 = Activation(activation='softmax')(batch_n_3)\n    \n    model = Model(inputs=image, outputs=a_3)\n    \n    adam_apt = keras.optimizers.Adam(lr=0.002, beta_1=0.99, beta_2=0.99, epsilon=1e-08)\n    \n    model.compile(loss='sparse_categorical_crossentropy',\n                   optimizer=adam_apt,\n                   metrics=['accuracy'])\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-17T03:49:10.868976Z","iopub.execute_input":"2022-04-17T03:49:10.869325Z","iopub.status.idle":"2022-04-17T03:49:10.888424Z","shell.execute_reply.started":"2022-04-17T03:49:10.869289Z","shell.execute_reply":"2022-04-17T03:49:10.887798Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### 2. Define model callbacks","metadata":{}},{"cell_type":"code","source":"my_callbacks = [\n    ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=1e-5, patience=10, verbose=1),\n    ModelCheckpoint('model_best.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Train Test split (80-20)","metadata":{}},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(images,\n                                                    truth_labels,\n                                                    shuffle=True,\n                                                    train_size=0.8,\n                                                    random_state=RANDOM_STATE\n                                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Image augmentation","metadata":{}},{"cell_type":"code","source":"gen = ImageDataGenerator(\n        rotation_range=360.,\n        horizontal_flip=True,\n        vertical_flip=True,\n        width_shift_range=0.3,\n        height_shift_range=0.3\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Create model and train","metadata":{}},{"cell_type":"code","source":"model_1 = create_model()\nhistory = model_1.fit_generator(gen.flow(x_train, y_train,batch_size=BATCH_SIZE),\n           epochs=EPOCHS,\n           verbose=1,\n           shuffle=True,\n           validation_data=(x_val, y_val),\n           callbacks=my_callbacks\n                               )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing model summary of the saved model\nplot_model(model_1, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T03:50:27.773024Z","iopub.execute_input":"2022-04-17T03:50:27.775437Z","iopub.status.idle":"2022-04-17T03:50:28.139262Z","shell.execute_reply.started":"2022-04-17T03:50:27.775393Z","shell.execute_reply":"2022-04-17T03:50:28.138358Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### 6. Plot accuracy and loss graphs","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label= 'train')\nplt.plot(history.history['val_accuracy'], label = 'val')\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label = 'train')\nplt.plot(history.history['val_loss'], label = 'val')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Load best model and predict on it","metadata":{}},{"cell_type":"code","source":"# load model\nmodel_best = load_model('model_best.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predictions on test data\nprob = model_best.predict(images_test, verbose=1)\npred = prob.argmax(axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# map the number labels back to the text labels\ntext_predictions = []\nfor i in pred:\n    text_predictions.append(num_label_to_text_label_dict[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save to df\ndf = pd.DataFrame({'file': os.listdir('../input/plant-seedlings-classification/test'), 'species': text_predictions})\ndf.to_csv('model_results.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # save to df\n# df = pd.DataFrame({'file': os.listdir('../input/plant-seedling-segmented/plant-seedling-segmented/seg_test'), 'species': text_predictions})\n# df.to_csv('model_results.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Confusion matrix for evaluation data","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=False):\n    if cmap is None:\n        cmap = plt.get_cmap('Oranges')\n\n    plt.figure(figsize=(12, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    \n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylim(len(target_names)-0.5, -0.5)\n    plt.ylabel('True labels')\n    plt.xlabel('Predicted labels')\n    plt.savefig(title + '.png', dpi=500, bbox_inches = 'tight')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predictions on eval dataset\ny_predict=model_best.predict(x_val)\ny_pred = y_predict.argmax(axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:42:24.724795Z","iopub.execute_input":"2022-03-29T09:42:24.725252Z","iopub.status.idle":"2022-03-29T09:43:36.598944Z","shell.execute_reply.started":"2022-03-29T09:42:24.725205Z","shell.execute_reply":"2022-03-29T09:43:36.598053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get list of text labels\ntext_label_list = []\nfor i in num_label_to_text_label_dict:\n    text_label_list.append(num_label_to_text_label_dict[i])\ntext_label_list","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:50:30.550035Z","iopub.execute_input":"2022-03-29T09:50:30.550801Z","iopub.status.idle":"2022-03-29T09:50:30.554856Z","shell.execute_reply.started":"2022-03-29T09:50:30.550747Z","shell.execute_reply":"2022-03-29T09:50:30.554068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# call the confusion_matrix function to build a confusion matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:51:52.088688Z","iopub.execute_input":"2022-03-29T09:51:52.089115Z","iopub.status.idle":"2022-03-29T09:51:52.098556Z","shell.execute_reply.started":"2022-03-29T09:51:52.08908Z","shell.execute_reply":"2022-03-29T09:51:52.097892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the cm in a pretty format\nplot_confusion_matrix(cm, text_label_list, \"CNN model Confusion Matrix\")","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:53:34.257848Z","iopub.execute_input":"2022-03-29T09:53:34.258142Z","iopub.status.idle":"2022-03-29T09:53:39.50674Z","shell.execute_reply.started":"2022-03-29T09:53:34.258108Z","shell.execute_reply":"2022-03-29T09:53:39.505783Z"},"trusted":true},"execution_count":null,"outputs":[]}]}